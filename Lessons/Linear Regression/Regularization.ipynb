{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization in Regression\n",
    "\n",
    "In this section, we will cover the concept of regularization in regression, addressing three main challenges: non-uniqueness, overfitting, and ill-conditioning. We will also discuss how regularization helps to alleviate these issues.\n",
    "\n",
    "#### Least-Squares Problem\n",
    "\n",
    "The least-squares problem uses a collection of data points $\\{\\mathbf{x}_n, y_n\\}$ to determine an optimal parameter $\\mathbf{w}$ by minimizing an empirical quadratic risk of the form:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = \\arg\\min_{\\mathbf{w}} \\left( \\frac{1}{N} \\sum_{n=1}^{N} \\left( y_n - \\mathbf{x}_n^T \\mathbf{w} \\right)^2 \\right)\n",
    "$$\n",
    "\n",
    "where each $y_n$ is a scalar and each $\\mathbf{x}_n$ is an $M$-dimensional vector. The solution is determined by solving the normal equations:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T \\mathbf{X} \\mathbf{w}^* = \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "where the design matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times M}$ and the vector $\\mathbf{y} \\in \\mathbb{R}^N$ collect the data:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    "\\mathbf{x}_1^T \\\\\n",
    "\\mathbf{x}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_N^T\n",
    "\\end{pmatrix}, \\quad\n",
    "\\mathbf{y} = \n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Training and Testing Errors\n",
    "\n",
    "Once a solution $\\mathbf{w}^*$ is determined, the value of the risk function at the solution is called the training error:\n",
    "\n",
    "$$\n",
    "\\text{Training error} = \\frac{1}{N} \\sum_{n=1}^{N} \\left( y_n - \\mathbf{x}_n^T \\mathbf{w}^* \\right)^2\n",
    "$$\n",
    "\n",
    "To assess the performance on future data, we define the testing error on a separate collection of $T$ test data points $\\{\\mathbf{x}_t, y_t\\}$:\n",
    "\n",
    "$$\n",
    "\\text{Testing error} = \\frac{1}{T} \\sum_{t=1}^{T} \\left( y_t - \\mathbf{x}_t^T \\mathbf{w}^* \\right)^2\n",
    "$$\n",
    "\n",
    "A learning algorithm that leads to a small gap between training and testing errors is said to generalize well.\n",
    "\n",
    "#### Challenges in Learning Problems\n",
    "\n",
    "##### Non-Uniqueness\n",
    "\n",
    "When the normal equations have infinitely many solutions, the training error remains invariant regardless of which solution is chosen, as they differ by vectors in the null space of $\\mathbf{X}$. However, the testing error may vary, leading to poor generalization.\n",
    "\n",
    "To address this, we employ $\\ell_2$-regularization (Ridge Regression), which forces a unique solution by adding a penalty term to the optimization problem:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = \\arg\\min_{\\mathbf{w}} \\left( \\frac{1}{N} \\sum_{n=1}^{N} \\left( y_n - \\mathbf{x}_n^T \\mathbf{w} \\right)^2 + \\lambda \\|\\mathbf{w}\\|_2^2 \\right)\n",
    "$$\n",
    "\n",
    "##### Overfitting\n",
    "\n",
    "Overfitting occurs when the model is too complex, fitting the training data very well but performing poorly on test data. This is common when the design matrix $\\mathbf{X}$ is rank-deficient.\n",
    "\n",
    "To mitigate overfitting, we use $\\ell_1$-regularization (Lasso Regression), which promotes sparsity in the solution:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = \\arg\\min_{\\mathbf{w}} \\left( \\frac{1}{N} \\sum_{n=1}^{N} \\left( y_n - \\mathbf{x}_n^T \\mathbf{w} \\right)^2 + \\lambda \\|\\mathbf{w}\\|_1 \\right)\n",
    "$$\n",
    "\n",
    "##### Ill-Conditioning\n",
    "\n",
    "Ill-conditioning occurs when small changes in the data lead to large changes in the solution, often due to large discrepancies in the magnitudes of entries in $\\mathbf{X}$.\n",
    "\n",
    "To handle ill-conditioning, we normalize the observation vectors to have unit variance and employ $\\ell_2$-regularization:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_n \\leftarrow \\frac{\\mathbf{x}_n - \\bar{\\mathbf{x}}}{\\sigma_{\\mathbf{x}}}\n",
    "$$\n",
    "\n",
    "where $\\bar{\\mathbf{x}}$ is the mean and $\\sigma_{\\mathbf{x}}$ is the standard deviation of $\\mathbf{x}_n$.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "One useful technique to avoid the challenges of nonuniqueness of solutions, overfitting, and ill-conditioning is to employ **regularization** (also called shrinkage in the statistics literature). The technique penalizes some norm of the parameter vector $\\mathbf{w}$ in order to favor solutions with desirable properties based on some prior knowledge (such as sparse solutions or solutions with a small Euclidean norm).\n",
    "\n",
    "Regularization incorporates a form of **inductive bias** by incorporating prior information into the model. This bias shifts the solution away from the unregularized case to include some form of penalty that reflects our prior beliefs or requirements. This is achieved by adding an explicit convex penalty term to the original risk function. The penalty term can take different forms, such as:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{w}) = \n",
    "\\begin{cases} \n",
    "\\rho \\|\\mathbf{w}\\|_2^2 & (\\ell_2\\text{-regularization}) \\\\\n",
    "\\alpha \\|\\mathbf{w}\\|_1 & (\\ell_1\\text{-regularization}) \\\\\n",
    "\\alpha \\|\\mathbf{w}\\|_1 + \\rho \\|\\mathbf{w}\\|_2^2 & (\\text{elastic-net regularization}) \\\\\n",
    "\\beta \\|\\mathbf{w}\\|_0 & (\\ell_0\\text{-regularization})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $(\\alpha, \\beta, \\rho)$ are nonnegative parameters, and $\\|\\mathbf{w}\\|_0$ is a pseudo-norm that counts the number of nonzero elements in $\\mathbf{w}$.\n",
    "\n",
    "In the following notebooks, we will delve into specific types of regularization, namely $\\ell_2$-regularization (Ridge Regression), $\\ell_1$-regularization (Lasso), and the combined approach known as Elastic-Net Regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
