{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel-Based Principal Component Analysis (Kernel PCA)\n",
    "\n",
    "Kernel Principal Component Analysis (Kernel PCA) is an extension of the traditional PCA algorithm that allows for nonlinear dimensionality reduction by using kernel methods. The goal of Kernel PCA is to perform dimensionality reduction in a higher-dimensional feature space implicitly defined by a kernel function, without the need to explicitly compute the transformed features.\n",
    "\n",
    "## Kernel Function and Gram Matrix\n",
    "\n",
    "Consider $N$ feature vectors $\\{\\mathbf{x}_n\\}$ in $\\mathbb{R}^M$ and a kernel function $K(\\mathbf{x}_n, \\mathbf{x}_m)$ that computes the inner product in the transformed feature space:\n",
    "\n",
    "$$\n",
    " K(\\mathbf{x}_n, \\mathbf{x}_m) = (\\phi(\\mathbf{x}_n))^T \\phi(\\mathbf{x}_m)\n",
    "$$\n",
    "\n",
    "for some mapping $\\phi(\\cdot)$. The corresponding $N \\times N$ Gram matrix $\\mathbf{A}$ is defined as:\n",
    "\n",
    "$$\n",
    "[\\mathbf{A}]_{n,m} = K(\\mathbf{x}_n, \\mathbf{x}_m)\n",
    "$$\n",
    "\n",
    "The matrix $\\mathbf{A}$ is symmetric and non-negative definite. The aim is to replace each transformed vector $\\phi(\\mathbf{x}_n)$ with a reduced vector $\\phi'(\\mathbf{x}_n)$ in $\\mathbb{R}^{M'}$ where $M' \\ll M$.\n",
    "\n",
    "## Centering the Data\n",
    "\n",
    "To perform Kernel PCA, we start by centering the transformed feature vectors. The sample mean of the transformed vectors is:\n",
    "\n",
    "$$ \n",
    "\\bar{\\phi} = \\frac{1}{N} \\sum_{n=0}^{N-1} \\phi(\\mathbf{x}_n)\n",
    "$$\n",
    "\n",
    "The centered transformed vectors are:\n",
    "\n",
    "$$\n",
    "\\phi_c(\\mathbf{x}_n) = \\phi(\\mathbf{x}_n) - \\bar{\\phi}, \\quad n = 0, 1, 2, \\ldots, N-1\n",
    "$$\n",
    "\n",
    "The Gram matrix of the centered variables, denoted by $\\mathbf{A}_c$, can be related to the original Gram matrix $\\mathbf{A}$ as follows:\n",
    "\n",
    "$$\n",
    "[\\mathbf{A}_c]_{n,m} = K(\\mathbf{x}_n, \\mathbf{x}_m) - \\frac{1}{N} \\sum_{k=0}^{N-1} K(\\mathbf{x}_n, \\mathbf{x}_k) - \\frac{1}{N} \\sum_{k=0}^{N-1} K(\\mathbf{x}_k, \\mathbf{x}_m) + \\frac{1}{N^2} \\sum_{k=0}^{N-1} \\sum_{j=0}^{N-1} K(\\mathbf{x}_k, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "## Eigenvalue Decomposition\n",
    "\n",
    "We then proceed with dimensionality reduction in the kernel domain. The sample covariance matrix of the centered and transformed feature vectors is:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{R}} = \\frac{1}{N-1} \\sum_{n=0}^{N-1} \\phi_c(\\mathbf{x}_n) \\phi_c(\\mathbf{x}_n)^T\n",
    "$$\n",
    "\n",
    "We consider its eigen-decomposition:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{R}} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^T\n",
    "$$\n",
    "\n",
    "where $\\mathbf{U}$ is orthogonal and $\\mathbf{\\Lambda}$ is diagonal with non-negative entries. The challenge is that we cannot directly compute the matrix $\\hat{\\mathbf{R}}$ because it requires the explicit computation of the transformed features $\\phi(\\mathbf{x}_n)$. Instead, we solve the eigenvalue problem using the Gram matrix $\\mathbf{A}_c$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Kernel PCA Algorithm\n",
    "\n",
    "1. **Compute the Gram matrix**:\n",
    "   $$ \n",
    "   [\\mathbf{A}]_{n,m} = K(\\mathbf{x}_n, \\mathbf{x}_m)\n",
    "   $$\n",
    "\n",
    "2. **Center the Gram matrix**:\n",
    "   $$\n",
    "   \\mathbf{A}_c = \\mathbf{A} - \\frac{1}{N} \\mathbf{A} \\mathbf{1}_N - \\frac{1}{N} \\mathbf{1}_N \\mathbf{A} + \\frac{1}{N^2} \\mathbf{1}_N \\mathbf{A} \\mathbf{1}_N\n",
    "   $$\n",
    "\n",
    "3. **Compute the eigenvalues and eigenvectors of the centered Gram matrix**:\n",
    "   Solve $\\mathbf{A}_c \\mathbf{a} = \\lambda \\mathbf{a}$\n",
    "\n",
    "4. **Construct the reduced features**:\n",
    "   For each feature vector $\\mathbf{x}_n$, compute the reduced features using the eigenvectors $\\mathbf{a}_k$:\n",
    "   $$\n",
    "   \\phi'_k(\\mathbf{x}_n) = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{m=0}^{N-1} a_k(m) K_c(\\mathbf{x}_n, \\mathbf{x}_m)\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "Kernel PCA provides a powerful method for nonlinear dimensionality reduction by leveraging kernel functions. This approach allows us to perform PCA in a high-dimensional feature space without explicitly computing the transformed features, enabling us to capture complex patterns and structures in the data.\n",
    "\n",
    "In the next steps, we will implement the Kernel PCA algorithm using Python and demonstrate its application on a non-linear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Fetch the dataset\n",
    "satimages = fetch_openml(data_id=182, as_frame=True)\n",
    "X, y = satimages.data, satimages.target\n",
    "\n",
    "# Display dataset information\n",
    "print(X.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8639191290824261\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       289\n",
      "           1       0.97      0.92      0.95       160\n",
      "           2       0.90      0.94      0.92       270\n",
      "           3       0.64      0.41      0.50       139\n",
      "           4       0.80      0.77      0.78       136\n",
      "           5       0.80      0.90      0.85       292\n",
      "\n",
      "    accuracy                           0.86      1286\n",
      "   macro avg       0.84      0.82      0.83      1286\n",
      "weighted avg       0.86      0.86      0.86      1286\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing\n",
    "numeric_features        = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features    = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "y = satimages.target\n",
    "label_mapping = {label: i for i, label in enumerate(np.unique(y))}\n",
    "y_numeric = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y_numeric, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline with KernelPCA and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('kpca', KernelPCA(n_components=150, kernel='sigmoid', gamma=None)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
