{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel-Based Principal Component Analysis (Kernel PCA)\n",
    "\n",
    "Kernel Principal Component Analysis (Kernel PCA) is an extension of the traditional PCA algorithm that allows for nonlinear dimensionality reduction by using kernel methods. The goal of Kernel PCA is to perform dimensionality reduction in a higher-dimensional feature space implicitly defined by a kernel function, without the need to explicitly compute the transformed features.\n",
    "\n",
    "## Kernel Function and Gram Matrix\n",
    "\n",
    "Consider $N$ feature vectors $\\{\\mathbf{x}_n\\}$ in $\\mathbb{R}^M$ and a kernel function $K(\\mathbf{x}_n, \\mathbf{x}_m)$ that computes the inner product in the transformed feature space:\n",
    "\n",
    "$$\n",
    " K(\\mathbf{x}_n, \\mathbf{x}_m) = (\\phi(\\mathbf{x}_n))^T \\phi(\\mathbf{x}_m)\n",
    "$$\n",
    "\n",
    "for some mapping $\\phi(\\cdot)$. The corresponding $N \\times N$ Gram matrix $\\mathbf{A}$ is defined as:\n",
    "\n",
    "$$\n",
    "[\\mathbf{A}]_{n,m} = K(\\mathbf{x}_n, \\mathbf{x}_m)\n",
    "$$\n",
    "\n",
    "The matrix $\\mathbf{A}$ is symmetric and non-negative definite. The aim is to replace each transformed vector $\\phi(\\mathbf{x}_n)$ with a reduced vector $\\phi'(\\mathbf{x}_n)$ in $\\mathbb{R}^{M'}$ where $M' \\ll M$.\n",
    "\n",
    "## Centering the Data\n",
    "\n",
    "To perform Kernel PCA, we start by centering the transformed feature vectors. The sample mean of the transformed vectors is:\n",
    "\n",
    "$$ \n",
    "\\bar{\\phi} = \\frac{1}{N} \\sum_{n=0}^{N-1} \\phi(\\mathbf{x}_n)\n",
    "$$\n",
    "\n",
    "The centered transformed vectors are:\n",
    "\n",
    "$$\n",
    "\\phi_c(\\mathbf{x}_n) = \\phi(\\mathbf{x}_n) - \\bar{\\phi}, \\quad n = 0, 1, 2, \\ldots, N-1\n",
    "$$\n",
    "\n",
    "The Gram matrix of the centered variables, denoted by $\\mathbf{A}_c$, can be related to the original Gram matrix $\\mathbf{A}$ as follows:\n",
    "\n",
    "$$\n",
    "[\\mathbf{A}_c]_{n,m} = K(\\mathbf{x}_n, \\mathbf{x}_m) - \\frac{1}{N} \\sum_{k=0}^{N-1} K(\\mathbf{x}_n, \\mathbf{x}_k) - \\frac{1}{N} \\sum_{k=0}^{N-1} K(\\mathbf{x}_k, \\mathbf{x}_m) + \\frac{1}{N^2} \\sum_{k=0}^{N-1} \\sum_{j=0}^{N-1} K(\\mathbf{x}_k, \\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "## Eigenvalue Decomposition\n",
    "\n",
    "We then proceed with dimensionality reduction in the kernel domain. The sample covariance matrix of the centered and transformed feature vectors is:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{R}} = \\frac{1}{N-1} \\sum_{n=0}^{N-1} \\phi_c(\\mathbf{x}_n) \\phi_c(\\mathbf{x}_n)^T\n",
    "$$\n",
    "\n",
    "We consider its eigen-decomposition:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{R}} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^T\n",
    "$$\n",
    "\n",
    "where $\\mathbf{U}$ is orthogonal and $\\mathbf{\\Lambda}$ is diagonal with non-negative entries. The challenge is that we cannot directly compute the matrix $\\hat{\\mathbf{R}}$ because it requires the explicit computation of the transformed features $\\phi(\\mathbf{x}_n)$. Instead, we solve the eigenvalue problem using the Gram matrix $\\mathbf{A}_c$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Kernel PCA Algorithm\n",
    "\n",
    "1. **Compute the Gram matrix**:\n",
    "   $$ \n",
    "   [\\mathbf{A}]_{n,m} = K(\\mathbf{x}_n, \\mathbf{x}_m)\n",
    "   $$\n",
    "\n",
    "2. **Center the Gram matrix**:\n",
    "   $$\n",
    "   \\mathbf{A}_c = \\mathbf{A} - \\frac{1}{N} \\mathbf{A} \\mathbf{1}_N - \\frac{1}{N} \\mathbf{1}_N \\mathbf{A} + \\frac{1}{N^2} \\mathbf{1}_N \\mathbf{A} \\mathbf{1}_N\n",
    "   $$\n",
    "\n",
    "3. **Compute the eigenvalues and eigenvectors of the centered Gram matrix**:\n",
    "   Solve $\\mathbf{A}_c \\mathbf{a} = \\lambda \\mathbf{a}$\n",
    "\n",
    "4. **Construct the reduced features**:\n",
    "   For each feature vector $\\mathbf{x}_n$, compute the reduced features using the eigenvectors $\\mathbf{a}_k$:\n",
    "   $$\n",
    "   \\phi'_k(\\mathbf{x}_n) = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{m=0}^{N-1} a_k(m) K_c(\\mathbf{x}_n, \\mathbf{x}_m)\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "Kernel PCA provides a powerful method for nonlinear dimensionality reduction by leveraging kernel functions. This approach allows us to perform PCA in a high-dimensional feature space without explicitly computing the transformed features, enabling us to capture complex patterns and structures in the data.\n",
    "\n",
    "In the next steps, we will implement the Kernel PCA algorithm using Python and demonstrate its application on a non-linear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6430 entries, 0 to 6429\n",
      "Data columns (total 36 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Aattr    6430 non-null   float64\n",
      " 1   Battr    6430 non-null   float64\n",
      " 2   Cattr    6430 non-null   float64\n",
      " 3   Dattr    6430 non-null   float64\n",
      " 4   Eattr    6430 non-null   float64\n",
      " 5   Fattr    6430 non-null   float64\n",
      " 6   A1attr   6430 non-null   float64\n",
      " 7   B2attr   6430 non-null   float64\n",
      " 8   C3attr   6430 non-null   float64\n",
      " 9   D4attr   6430 non-null   float64\n",
      " 10  E5attr   6430 non-null   float64\n",
      " 11  F6attr   6430 non-null   float64\n",
      " 12  A7attr   6430 non-null   float64\n",
      " 13  B8attr   6430 non-null   float64\n",
      " 14  C9attr   6430 non-null   float64\n",
      " 15  D10attr  6430 non-null   float64\n",
      " 16  E11attr  6430 non-null   float64\n",
      " 17  F12attr  6430 non-null   float64\n",
      " 18  A13attr  6430 non-null   float64\n",
      " 19  B14attr  6430 non-null   float64\n",
      " 20  C15attr  6430 non-null   float64\n",
      " 21  D16attr  6430 non-null   float64\n",
      " 22  E17attr  6430 non-null   float64\n",
      " 23  F18attr  6430 non-null   float64\n",
      " 24  A19attr  6430 non-null   float64\n",
      " 25  B20attr  6430 non-null   float64\n",
      " 26  C21attr  6430 non-null   float64\n",
      " 27  D22attr  6430 non-null   float64\n",
      " 28  E23attr  6430 non-null   float64\n",
      " 29  F24attr  6430 non-null   float64\n",
      " 30  A25attr  6430 non-null   float64\n",
      " 31  B26attr  6430 non-null   float64\n",
      " 32  C27attr  6430 non-null   float64\n",
      " 33  D28attr  6430 non-null   float64\n",
      " 34  E29attr  6430 non-null   float64\n",
      " 35  F30attr  6430 non-null   float64\n",
      "dtypes: float64(36)\n",
      "memory usage: 1.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Fetch the dataset\n",
    "satimages = fetch_openml(data_id=182, as_frame=True)\n",
    "X, y = satimages.data, satimages.target\n",
    "\n",
    "# Display dataset information\n",
    "print(X.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8950233281493002\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       289\n",
      "           1       0.98      0.97      0.98       160\n",
      "           2       0.91      0.94      0.93       270\n",
      "           3       0.70      0.58      0.63       139\n",
      "           4       0.91      0.82      0.86       136\n",
      "           5       0.84      0.90      0.87       292\n",
      "\n",
      "    accuracy                           0.90      1286\n",
      "   macro avg       0.88      0.87      0.87      1286\n",
      "weighted avg       0.89      0.90      0.89      1286\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing\n",
    "numeric_features        = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features    = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "y = satimages.target\n",
    "label_mapping = {label: i for i, label in enumerate(np.unique(y))}\n",
    "y_numeric = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y_numeric, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline with KernelPCA and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('kpca', KernelPCA(n_components=5000, kernel='rbf', gamma=None)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
