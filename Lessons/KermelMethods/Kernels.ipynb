{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "\n",
    "Hay tres desafíos principales con las transformaciones del tipo $\\phi(\\mathbf{x})$:\n",
    "\n",
    "1. **Selección de la Transformación**: El diseñador no sabe de antemano qué transformación $\\phi(\\cdot)$ seleccionar.\n",
    "2. **Cálculo de Productos Internos**: La mayoría de los algoritmos de aprendizaje, tanto en línea como por lotes, requieren el cálculo de productos internos entre vectores de características y pesos. Esto puede ser problemático para espacios de alta dimensión con grandes $M'$.\n",
    "3. **Formación de Vectores Transformados**: Una vez seleccionada la transformación $\\phi(\\cdot)$, aún es necesario formar los vectores transformados $\\phi(\\mathbf{x}_n)$.\n",
    "\n",
    "Los métodos kernels proporcionan una forma elegante de superar estas dificultades. Estos métodos permiten extender el espacio de características y realizar los cálculos necesarios de manera eficiente y flexible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Definición de Kernel\n",
    "\n",
    "Un kernel es una función que mapea dos argumentos vectoriales al producto interno de versiones transformadas de estos vectores, es decir,\n",
    "\n",
    "$$ \n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) : \\mathbb{R}^M \\times \\mathbb{R}^M \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\phi(\\mathbf{x}_i))^T \\phi(\\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "para alguna función $\\phi(\\cdot)$. Es importante notar que los núcleos son funciones simétricas, es decir,\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = K(\\mathbf{x}_j, \\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "Decimos que las funciones kernel realizan operaciones de producto interno en el dominio transformado. No todas las funciones $K(\\mathbf{x}_i, \\mathbf{x}_j)$ pueden expresarse en la forma de producto interno anterior, y por lo tanto, no todas las funciones son núcleos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Teorema de Mercer\n",
    "\n",
    "Un teorema fundamental en análisis funcional, conocido como el teorema de Mercer, aclara qué funciones $K(\\mathbf{x}_i, \\mathbf{x}_j)$ pueden expresarse en la forma de producto interno. Para cualquier entero $N$, introducimos la siguiente matriz **Gramiana** $A_N$, que es simétrica:\n",
    "\n",
    "$$\n",
    "[A_N]_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j), \\quad i, j = 0, 1, 2, \\ldots, N-1\n",
    "$$\n",
    "\n",
    "**Teorema de Mercer**: Una función $K(\\mathbf{x}_i, \\mathbf{x}_j)$ que sea simétrica y cuadráticamente integrable es un kernel si, y solo si, la matriz Gramiana $A_N$ definida anteriormente es semidefinida positiva para cualquier tamaño $N$ y cualquier conjunto de datos de características $\\{\\mathbf{x}_n\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Demostración\n",
    "\n",
    "Asumimos primero que $K(\\cdot, \\cdot)$ es un kernel. Recogemos los vectores de características transformados en la matriz:\n",
    "\n",
    "$$\n",
    "H' = [\\phi(\\mathbf{x}_0) \\ \\phi(\\mathbf{x}_1) \\ \\ldots \\ \\phi(\\mathbf{x}_{N-1})]\n",
    "$$\n",
    "\n",
    "Entonces, se cumple que $A_N = H'^T H'$, de modo que $A_N \\succeq 0$. Inversamente, asumimos que $A_N \\succeq 0$ para cualquier $N$ y $\\{\\mathbf{x}_n\\}$. Entonces, $A_N$ admite una descomposición espectral de la forma $A_N = V^T \\Lambda V$.\n",
    "\n",
    "Sea $V = [\\mathbf{v}_0 \\ \\mathbf{v}_1 \\ \\ldots \\ \\mathbf{v}_{N-1}]$ y definimos $\\phi(\\mathbf{x}_n) = \\mathbf{v}_n$. Entonces, la entrada $(i, j)$ de $A_N$ es igual al producto interno $(\\phi(\\mathbf{x}_i))^T \\phi(\\mathbf{x}_j)$, que por la definición anterior es también igual a $K(\\mathbf{x}_i, \\mathbf{x}_j)$. Por lo tanto, se cumple que\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\phi(\\mathbf{x}_i))^T \\phi(\\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "para alguna función $\\phi(\\cdot)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ejemplo: kernel Polinomial de Segundo Orden\n",
    "\n",
    "Consideremos la siguiente función:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (1 + \\mathbf{x}_i^T \\mathbf{x}_j)^2\n",
    "$$\n",
    "\n",
    "que mapea dos vectores de características $(\\mathbf{x}_i, \\mathbf{x}_j)$ en un escalar no negativo. Queremos verificar que la función definida de esta manera es un kernel. Para hacerlo, y de acuerdo con la definición anterior, necesitamos identificar una transformación $\\phi(\\mathbf{x})$ que nos permita expresar $K(\\mathbf{x}_i, \\mathbf{x}_j)$ como el producto interno $(\\phi(\\mathbf{x}_i))^T \\phi(\\mathbf{x}_j)$. \n",
    "\n",
    "Nos enfocamos en el caso $M = 2$. Denotamos las entradas individuales de $(\\mathbf{x}_i, \\mathbf{x}_j)$ por:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i = \\begin{pmatrix} x_{i1} \\\\ x_{i2} \\end{pmatrix}, \\quad \\mathbf{x}_j = \\begin{pmatrix} x_{j1} \\\\ x_{j2} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Entonces, utilizando $K(\\mathbf{x}_i, \\mathbf{x}_j)$, tenemos:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (1 + x_{i1} x_{j1} + x_{i2} x_{j2})^2\n",
    "$$\n",
    "\n",
    "Lo que podemos expresar más compactamente introduciendo los vectores transformados:\n",
    "\n",
    "$$\\phi(\\mathbf{x}_i) = \\begin{pmatrix} 1 \\\\ \\sqrt{2} x_{i1} \\\\ \\sqrt{2} x_{i2} \\\\ \\sqrt{2} x_{i1} x_{i2} \\\\ x_{i1}^2 \\\\ x_{i2}^2 \\end{pmatrix}, \\quad \\phi(\\mathbf{x}_j) = \\begin{pmatrix} 1 \\\\ \\sqrt{2} x_{j1} \\\\ \\sqrt{2} x_{j2} \\\\ \\sqrt{2} x_{j1} x_{j2} \\\\ x_{j1}^2 \\\\ x_{j2}^2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "y entonces:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\phi(\\mathbf{x}_i))^T \\phi(\\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "Este ejemplo ilustra que la evaluación de $K(\\mathbf{x}_i, \\mathbf{x}_j)$ no requiere formar primero los vectores transformados $(\\phi(\\mathbf{x}_i), \\phi(\\mathbf{x}_j))$. En su lugar, la función kernel puede evaluarse directamente en el dominio de características original utilizando la fórmula proporcionada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Núcleo Polinomial de Orden $p$\n",
    "\n",
    "En general, un núcleo polinomial de orden $p$ toma la forma:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = (1 + \\mathbf{x}_i^T \\mathbf{x}_j)^p, \\quad p = 1, 2, 3, \\ldots\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Gaussiano\n",
    "\n",
    "Otro ejemplo popular de núcleo es el núcleo Gaussiano (también llamado función de base radial (RBF) o núcleo exponencial cuadrado). Se define como:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp \\left( -\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "para algún parámetro $\\sigma^2 > 0$. Este parámetro controla el ancho del pulso Gaussiano.\n",
    "\n",
    "### Ejemplo de kernel Gaussiano\n",
    "\n",
    "Reescribimos el núcleo Gaussiano en la forma:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = e^{-\\frac{\\|\\mathbf{x}_i\\|^2}{2\\sigma^2}} e^{-\\frac{\\|\\mathbf{x}_j\\|^2}{2\\sigma^2}} e^{\\frac{\\mathbf{x}_i^T \\mathbf{x}_j}{\\sigma^2}}\n",
    "$$\n",
    "\n",
    "lo que expresa $K(\\mathbf{x}_i, \\mathbf{x}_j)$ como el producto de tres términos. Sea:\n",
    "\n",
    "$$\n",
    "K_1(\\mathbf{x}_i, \\mathbf{x}_j) = e^{-\\frac{\\|\\mathbf{x}_i\\|^2}{2\\sigma^2}} e^{-\\frac{\\|\\mathbf{x}_j\\|^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K_2(\\mathbf{x}_i, \\mathbf{x}_j) = e^{\\frac{\\mathbf{x}_i^T \\mathbf{x}_j}{\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Entonces, ambas funciones son núcleos. La función $K_1(\\mathbf{x}_i, \\mathbf{x}_j)$ es un núcleo porque se expresa como el producto de dos funciones de un solo argumento, como lo requiere la propiedad (d). De igual manera, $K_2(\\mathbf{x}_i, \\mathbf{x}_j)$ es un núcleo debido a la propiedad (f). Por lo tanto, $K(\\mathbf{x}_i, \\mathbf{x}_j)$ es un núcleo por la propiedad (b).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Kernels\n",
    "\n",
    "Given two valid kernels $k_1(\\mathbf x, \\mathbf x′)$ and $k_2(\\mathbf x, \\mathbf x′)$, we can create a new kernel using any of the following methods:\n",
    "1. $k(\\mathbf x, \\mathbf x′) = c\\cdot k_1(\\mathbf x, \\mathbf x′)$, for any constant $c > 0$.\n",
    "1. $k(\\mathbf x, \\mathbf x′) = f(\\mathbf x)\\cdot k_1(\\mathbf x,\\mathbf x′)\\cdot f(\\mathbf x′)$, for any function $f$\n",
    "1. $k(\\mathbf x, \\mathbf x′) = q(k_1(x, x′))$ for any function polynomial $q$ with nonneg. coef.\n",
    "1. $k(\\mathbf x, \\mathbf x′) = \\exp(k_1(\\mathbf x, \\mathbf x′))$\n",
    "1. $k(\\mathbf x, \\mathbf x′) = \\mathbf x^TA\\mathbf x′$, for any psd matrix $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusión\n",
    "\n",
    "Los kernels polinomiales y Gaussianos son herramientas poderosas en el aprendizaje automático, permitiendo manejar datos no lineales de manera eficiente al operar en espacios de características transformados implícitamente. Esto evita la necesidad de calcular explícitamente las transformaciones, simplificando y acelerando los cálculos en muchos algoritmos de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
