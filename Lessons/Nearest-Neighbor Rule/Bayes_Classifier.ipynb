{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Bayes Classifier for Minimum Error\n",
    "\n",
    "In the previous sections, we discussed general principles of prediction and loss functions, primarily in the context of regression where the target variable $\\mathbf{y}$ is continuous. Now, we turn our attention specifically to **classification problems**.\n",
    "\n",
    "In classification, we are given a feature vector $\\mathbf{x}$ and our goal is to assign it to one of $K$ discrete classes, which we can denote as $C_1, C_2, \\dots, C_K$. Let $t$ represent the true class label for a given $\\mathbf{x}$ (so $t \\in \\{C_1, \\dots, C_K\\}$). We want to find a decision rule, or a classifier, $f(\\mathbf{x})$, that takes an input $\\mathbf{x}$ and assigns it to one of the $K$ classes.\n",
    "\n",
    "The most fundamental goal in classification is often to **minimize the probability of misclassification**. This means we want to choose $f(\\mathbf{x})$ such that the probability $p(f(\\mathbf{x}) \\neq t)$ is as small as possible.\n",
    "\n",
    "The decision rule that minimizes this misclassification probability is known as the **Bayes classifier**. For each input $\\mathbf{x}$, the Bayes classifier assigns $\\mathbf{x}$ to the class $C_k$ that has the highest posterior probability $p(C_k|\\mathbf{x})$. That is:\n",
    "\n",
    "$$\n",
    "f_{\\text{Bayes}}(\\mathbf{x}) = \\underset{C_k \\in \\{C_1, \\dots, C_K\\}}{\\mathrm{argmax}} \\ p(C_k | \\mathbf{x})\n",
    "$$\n",
    "\n",
    "This is also known as the **Maximum A Posteriori (MAP)** estimation rule for the class label. If we achieve this for every $\\mathbf{x}$, we will have the lowest possible misclassification rate. This optimal misclassification rate is called the **Bayes error rate**.\n",
    "\n",
    "We can denote the ideal mapping corresponding to the Bayes classifier as $f^{\\bullet}(\\mathbf{x})$.\n",
    "\n",
    "### 7.1 The Challenge: Unknown Posterior Probabilities\n",
    "\n",
    "The main challenge is that the true posterior probabilities $p(C_k|\\mathbf{x})$ are generally unknown in practice. We typically only have a training dataset $\\mathcal{D}_{\\text{train}} = \\{(\\mathbf{x}_n, t_n)\\}_{n=1}^N$, where $t_n$ is the true class label for $\\mathbf{x}_n$.\n",
    "\n",
    "Therefore, we aim to find an **approximate classifier**, let's call it $f^*(\\mathbf{x})$, based on the training data. There are two main approaches to this:\n",
    "\n",
    "1.  **Discriminative Approach:**\n",
    "    *   Model the posterior probabilities $p(C_k|\\mathbf{x})$ directly.\n",
    "    *   Examples: Logistic Regression, Neural Networks (with softmax output).\n",
    "    *   Once $p(C_k|\\mathbf{x})$ is estimated, the MAP rule is applied.\n",
    "\n",
    "2.  **Generative Approach:**\n",
    "    *   Model the class-conditional densities $p(\\mathbf{x}|C_k)$ and the class priors $p(C_k)$.\n",
    "    *   Then, use Bayes' theorem to find the posterior probabilities:\n",
    "        $$\n",
    "        p(C_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|C_k)p(C_k)}{p(\\mathbf{x})} = \\frac{p(\\mathbf{x}|C_k)p(C_k)}{\\sum_{j=1}^K p(\\mathbf{x}|C_j)p(C_j)}\n",
    "        $$\n",
    "    *   Examples: Naive Bayes, Gaussian Discriminant Analysis.\n",
    "    *   Generative models can also be used to generate synthetic data points $\\mathbf{x}$ by sampling from $p(\\mathbf{x}|C_k)$ and $p(C_k)$.\n",
    "\n",
    "The methods we will explore in this series, such as k-Nearest Neighbors, often take a more direct, data-based approach to approximate the decision rule without explicitly forming full probabilistic models for these distributions.\n",
    "\n",
    "### 7.2 Evaluating Classifier Performance: Classification Errors\n",
    "\n",
    "When we learn a classifier $f^*(\\mathbf{x})$ from training data, we need ways to evaluate its performance.\n",
    "\n",
    "*   **Training Data vs. Test Data:**\n",
    "    *   The data used to learn $f^*(\\mathbf{x})$ is the **training data**, $\\mathcal{D}_{\\text{train}} = \\{(\\mathbf{x}_n, t_n)\\}_{n=1}^D$.\n",
    "    *   To assess how well the classifier generalizes to new, unseen data, we use a separate **test data** set, $\\mathcal{D}_{\\text{test}} = \\{(\\mathbf{x}_m, t_m)\\}_{m=1}^M$, which is assumed to be drawn from the same underlying distribution as the training data but was not used during the learning process.\n",
    "\n",
    "*   **Empirical Error on Training Data (Training Error Rate):**\n",
    "    This measures the fraction of misclassifications on the training set:\n",
    "    $$\n",
    "    R_{\\text{emp}}^{\\text{train}}(f^*) = \\frac{1}{D} \\sum_{n=1}^{D} I(f^*(\\mathbf{x}_n) \\neq t_n)\n",
    "    $$\n",
    "    where $I(\\cdot)$ is the **indicator function**, defined as:\n",
    "    $$\n",
    "    I(\\text{condition}) = \\begin{cases} 1 & \\text{if condition is true} \\\\ 0 & \\text{if condition is false} \\end{cases}\n",
    "    $$\n",
    "    The training error is often an overly optimistic estimate of how the classifier will perform on new data, as $f^*(\\mathbf{x})$ was optimized using this very data.\n",
    "\n",
    "*   **Empirical Error on Test Data (Test Error Rate):**\n",
    "    This measures the fraction of misclassifications on the test set:\n",
    "    $$\n",
    "    R_{\\text{emp}}^{\\text{test}}(f^*) = \\frac{1}{M} \\sum_{m=1}^{M} I(f^*(\\mathbf{x}_m) \\neq t_m)\n",
    "    $$\n",
    "    The test error provides a more realistic estimate of the classifier's performance on unseen data.\n",
    "\n",
    "*   **Generalization Error (True Error Rate):**\n",
    "    The ultimate measure of a classifier's performance is its **generalization error**, $R(f^*)$, which is the expected error over the true underlying data distribution $p(\\mathbf{x}, t)$:\n",
    "    $$\n",
    "    R(f^*) = \\mathbb{E}_{\\mathbf{x},t}[I(f^*(\\mathbf{x}) \\neq t)]\n",
    "    $$\n",
    "    This can also be expressed as the probability of making an erroneous decision:\n",
    "    $$\n",
    "    R(f^*) = p(f^*(\\mathbf{x}) \\neq t)\n",
    "    $$\n",
    "    In practice, we cannot compute $R(f^*)$ exactly because $p(\\mathbf{x}, t)$ is unknown. The test error $R_{\\text{emp}}^{\\text{test}}(f^*)$ serves as an estimate of the generalization error.\n",
    "\n",
    "*   **Generalization and Overfitting:**\n",
    "    A good learning algorithm should not only achieve a low training error but also generalize well to new data, meaning the test error should also be low and not significantly higher than the training error.\n",
    "    If a classifier performs very well on the training data (low $R_{\\text{emp}}^{\\text{train}}$) but poorly on the test data (high $R_{\\text{emp}}^{\\text{test}}$), it is said to have **overfit** the training data. This means it has learned the noise or specific idiosyncrasies of the training set rather than the true underlying patterns.\n",
    "\n",
    "    **Example: A Memorizing Classifier and Overfitting**\n",
    "    Consider a classifier $f_{\\text{mem}}(\\mathbf{x})$ that perfectly memorizes the training data:\n",
    "    $$\n",
    "    f_{\\text{mem}}(\\mathbf{x}) = \\begin{cases} t_n & \\text{if } \\mathbf{x} = \\mathbf{x}_n \\text{ for some } (\\mathbf{x}_n, t_n) \\in \\mathcal{D}_{\\text{train}} \\\\ \\text{a randomly chosen class} & \\text{if } \\mathbf{x} \\text{ is not in } \\mathcal{D}_{\\text{train}} \\end{cases}\n",
    "    $$\n",
    "    For this classifier:\n",
    "    *   The training error $R_{\\text{emp}}^{\\text{train}}(f_{\\text{mem}})$ will be 0 (or very close to 0 if there are conflicting labels for identical $\\mathbf{x}_n$).\n",
    "    *   However, its performance on test data $R_{\\text{emp}}^{\\text{test}}(f_{\\text{mem}})$ can be very poor, especially if the input space is continuous or high-dimensional, as new feature vectors $\\mathbf{x}_m$ are unlikely to exactly match any of the training vectors $\\mathbf{x}_n$. This is a classic example of overfitting.\n",
    "\n",
    "The crucial aspect of machine learning is to develop classifiers that generalize well to unseen data, capturing the underlying structure of the data distribution rather than just memorizing the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Helper function to plot decision boundaries (from scikit-learn documentation) ---\n",
    "def plot_decision_boundary(clf, X, y, ax, title):\n",
    "    h = .02  # step size in the mesh\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "# --- 1. Generate Synthetic Data ---\n",
    "# make_moons is good for showing non-linear boundaries\n",
    "# noise parameter introduces some overlap and makes it more realistic\n",
    "X, y = make_moons(n_samples=300, noise=0.6, random_state=42)\n",
    "\n",
    "# --- 2. Split into Training and Test sets ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# --- 3. Define Models ---\n",
    "# Model 1: A \"reasonable\" classifier (proxy for a good model)\n",
    "# Using KNeighborsClassifier with a moderate k\n",
    "reasonable_clf = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "# Model 2: A classifier prone to overfitting\n",
    "# KNeighborsClassifier with k=1 (memorizes training data)\n",
    "overfitting_clf = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# (Optional) Model 3: A simple linear model that might underfit\n",
    "# simple_clf = LogisticRegression(solver='liblinear', random_state=42) # Example\n",
    "\n",
    "classifiers = {\n",
    "    \"Reasonable (k=15 NN)\": reasonable_clf,\n",
    "    \"Overfitting (k=1 NN)\": overfitting_clf,\n",
    "    # \"Simple Linear (Logistic Regression)\": simple_clf\n",
    "}\n",
    "\n",
    "# --- 4. Train and Evaluate ---\n",
    "print(f\"{'Classifier':<35} | {'Train Accuracy':<15} | {'Test Accuracy':<15} | {'Train Error':<15} | {'Test Error':<15}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(classifiers), figsize=(5 * len(classifiers), 5))\n",
    "if len(classifiers) == 1: # Ensure axes is iterable\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (name, clf) in enumerate(classifiers.items()):\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Calculate error rates (1 - accuracy)\n",
    "    train_error = 1 - train_accuracy\n",
    "    test_error = 1 - test_accuracy\n",
    "\n",
    "    print(f\"{name:<35} | {train_accuracy:^15.3f} | {test_accuracy:^15.3f} | {train_error:^15.3f} | {test_error:^15.3f}\")\n",
    "\n",
    "    # Plot decision boundary\n",
    "    plot_decision_boundary(clf, X_train, y_train, axes[i], f\"{name}\\nTrain Acc: {train_accuracy:.2f}, Test Acc: {test_accuracy:.2f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDiscussion:\")\n",
    "print(\" - The 'Reasonable (k=15 NN)' classifier shows a good balance. Its training and test accuracies are relatively close, indicating good generalization.\")\n",
    "print(\" - The 'Overfitting (k=1 NN)' classifier achieves very high (often perfect) accuracy on the training data.\")\n",
    "print(\"   However, its test accuracy is noticeably lower. This gap between training and test performance is a hallmark of overfitting.\")\n",
    "print(\"   The decision boundary for k=1 NN will likely be very complex and 'jagged', trying to perfectly separate every training point.\")\n",
    "print(\" - If a 'Simple Linear' model was used on these non-linear 'moons' data, it would likely show underfitting:\")\n",
    "print(\"   both training and test accuracy would be relatively low because the model is too simple to capture the data's structure.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
