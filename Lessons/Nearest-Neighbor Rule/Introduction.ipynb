{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest-Neighbor Methods: I. Foundational Concepts\n",
    "\n",
    "This series of notebooks explores data-driven methods for inference and learning, with a particular focus on the Nearest-Neighbor rule and related concepts. We begin by laying down some foundational ideas from Bayesian decision theory and how they motivate different approaches to building predictive models.\n",
    "\n",
    "## 1. The Goal of Supervised Learning\n",
    "\n",
    "In supervised learning, our primary goal is to learn a mapping from an input (or feature) vector $\\mathbf{x}$ to an output (or observation/target) variable $\\mathbf{y}$. The nature of $\\mathbf{y}$ determines the type of problem:\n",
    "*   If $\\mathbf{y}$ is a continuous variable (or vector of continuous variables), it's a **regression** problem.\n",
    "*   If $\\mathbf{y}$ is a discrete variable representing categories or classes, it's a **classification** problem.\n",
    "\n",
    "We assume that the data is generated from some true, but unknown, probability distribution $p(\\mathbf{x}, \\mathbf{y})$. Our objective is to use a training dataset $\\mathcal{D} = \\{(\\mathbf{x}_n, \\mathbf{y}_n)\\}_{n=1}^D$, consisting of $D$ observations, to make predictions for new, unseen values of $\\mathbf{x}$.\n",
    "\n",
    "## 2. Bayesian Decision Theory for Prediction\n",
    "\n",
    "To make \"optimal\" predictions, Bayesian decision theory provides a framework.\n",
    "\n",
    "### 2.1 Loss Functions\n",
    "First, we need to specify a **loss function** $L(\\mathbf{y}, f(\\mathbf{x}))$ which quantifies the penalty or cost associated with predicting $f(\\mathbf{x})$ when the true observation is $\\mathbf{y}$. Our goal is to choose a prediction function $f(\\mathbf{x})$ that minimizes the average or **expected loss**:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[L] = \\iint L(\\mathbf{y}, f(\\mathbf{x})) p(\\mathbf{x}, \\mathbf{y}) d\\mathbf{x} d\\mathbf{y}\n",
    "$$\n",
    "\n",
    "### 2.2 Optimal Prediction for Squared Error Loss\n",
    "A commonly used loss function for regression problems is the **squared error loss**:\n",
    "$L(\\mathbf{y}, f(\\mathbf{x})) = ||f(\\mathbf{x}) - \\mathbf{y}||^2$.\n",
    "For this loss function, the optimal prediction $f_{\\text{opt}}(\\mathbf{x})$ that minimizes the expected loss is the **conditional expectation** of $\\mathbf{y}$ given $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "f_{\\text{opt}}(\\mathbf{x}) = \\mathbb{E}[\\mathbf{y}|\\mathbf{x}] = \\int \\mathbf{y} p(\\mathbf{y}|\\mathbf{x}) d\\mathbf{y}\n",
    "$$\n",
    "\n",
    "This function is also known as the **regression function**. To compute it, we need to know the conditional probability distribution $p(\\mathbf{y}|\\mathbf{x})$.\n",
    "\n",
    "### 2.3 The Challenge: Unknown Distributions\n",
    "In most real-world scenarios, the true joint distribution $p(\\mathbf{x}, \\mathbf{y})$ (and thus the conditional $p(\\mathbf{y}|\\mathbf{x})$ or the marginals) is unknown. We only have the training data $\\mathcal{D}$. This leads to two main strategies for tackling the prediction problem.\n",
    "\n",
    "## 3. Path 1: Parametric Models and Empirical Risk Minimization\n",
    "\n",
    "One common approach is to:\n",
    "1.  **Choose a parametric form for the prediction function**, $f(\\mathbf{x}; \\mathbf{w})$, where $\\mathbf{w}$ is a vector of adaptable parameters. A simple example is a linear model, $f(\\mathbf{x}; \\mathbf{w}) = \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})$, where $\\boldsymbol{\\phi}(\\mathbf{x})$ is a (possibly nonlinear) basis function expansion of the input $\\mathbf{x}$. (Note: we use $f(\\mathbf{x}; \\mathbf{w})$ to denote that $f$ is parameterized by $\\mathbf{w}$, distinct from $f(\\mathbf{x}, \\mathbf{w})$ which might imply $\\mathbf{w}$ is another input variable).\n",
    "2.  **Use the training data to determine the parameters** $\\mathbf{w}$. This is typically done by minimizing an **empirical risk**, which is an approximation of the true expected loss based on the training data. For the squared error loss, this leads to minimizing the **sum-of-squares error function**:\n",
    "\n",
    "    $$\n",
    "    E_D(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^{N} ||f(\\mathbf{x}_n; \\mathbf{w}) - \\mathbf{y}_n||^2\n",
    "    $$\n",
    "3.  **Add Regularization**: To control model complexity and prevent overfitting (where the model learns the training data too well but generalizes poorly to new data), a **regularization term** $\\Omega(\\mathbf{w})$ is often added to the error function. Common regularizers include L2 (ridge) and L1 (lasso) penalties:\n",
    "\n",
    "    $$\n",
    "    \\tilde{E}_D(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^{N} ||f(\\mathbf{x}_n; \\mathbf{w}) - \\mathbf{y}_n||^2 + \\lambda_2 ||\\mathbf{w}||_2^2 + \\lambda_1 ||\\mathbf{w}||_1\n",
    "    $$\n",
    "    Here, $\\lambda_1 \\ge 0$ and $\\lambda_2 \\ge 0$ are regularization coefficients that control the strength of the penalty. The term $ ||\\mathbf{w}||_2^2 = \\mathbf{w}^T\\mathbf{w}$ is the L2 norm (sum of squares of parameters), and $||\\mathbf{w}||_1 = \\sum_i |w_i|$ is the L1 norm (sum of absolute values of parameters).\n",
    "\n",
    "This approach transforms the learning problem into an optimization problem: finding the parameters $\\mathbf{w}^*$ that minimize $\\tilde{E}_D(\\mathbf{w})$. This is often solved using techniques like gradient descent or stochastic approximation algorithms.\n",
    "\n",
    "## 4. Path 2: Data-Based (Non-parametric) Inference\n",
    "\n",
    "An alternative strategy, particularly relevant to the methods we will discuss in this series, is to develop methods that:\n",
    "*   Directly estimate the conditional probability density $p(\\mathbf{y}|\\mathbf{x})$ or the joint density $p(\\mathbf{x},\\mathbf{y})$ from the training data.\n",
    "*   Or, more generally, infer the decision rule or prediction $f(\\mathbf{x})$ from the data without explicitly estimating a full density or a fixed set of parameters for a global model.\n",
    "\n",
    "These are often referred to as **non-parametric methods**. They typically make fewer assumptions about the form of the underlying distributions, and their complexity can grow with the size of the training data. The **Nearest-Neighbor (NN) rule** is a prime example of this approach.\n",
    "\n",
    "## 5. General Loss Functions and Optimal Decisions\n",
    "\n",
    "The Bayesian decision-theoretic framework extends beyond the squared error loss. For any general loss function $L(\\mathbf{y}, f(\\mathbf{x}))$, the optimal prediction $f_{\\text{opt}}(\\mathbf{x})$ is the one that minimizes the conditional expected loss for each $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "f_{\\text{opt}}(\\mathbf{x}) = \\underset{c}{\\mathrm{argmin}} \\int L(\\mathbf{y}, c) p(\\mathbf{y}|\\mathbf{x}) d\\mathbf{y}\n",
    "$$\n",
    "\n",
    "where $c$ represents a potential prediction value. Again, computing this optimal prediction requires knowledge of $p(\\mathbf{y}|\\mathbf{x})$.\n",
    "\n",
    "If we follow \"Path 1\" (parametric models) with a general loss function $L(\\mathbf{y}, f(\\mathbf{x}; \\mathbf{w}))$, the goal becomes finding parameters $\\mathbf{w}^*$ that minimize a regularized empirical risk of the form:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\mathrm{argmin}} \\left\\{ \\frac{1}{N} \\sum_{n=1}^{N} L(\\mathbf{y}_n, f(\\mathbf{x}_n; \\mathbf{w})) + \\Omega(\\mathbf{w}) \\right\\}\n",
    "$$\n",
    "where $\\Omega(\\mathbf{w})$ represents the regularization term (e.g., $\\lambda_2 ||\\mathbf{w}||_2^2 + \\lambda_1 ||\\mathbf{w}||_1$).\n",
    "\n",
    "## 6. Moving Forward\n",
    "\n",
    "In the subsequent notebooks, we will delve into \"Path 2,\" focusing on data-based methods that infer prediction rules more directly from the training samples. We will start by exploring the Nearest-Neighbor (NN) rule, which forms predictions based on the labels of the closest training examples in the feature space."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
