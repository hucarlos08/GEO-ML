{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Techniques in Regression: A Bayesian Perspective\n",
    "\n",
    "In this notebook, we will explore Ridge Regression, Lasso Regression, and Elastic Net from both a traditional and Bayesian perspective using the Maximum A Posteriori (MAP) formulation. \n",
    "## Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Bias-Variance Trade-Off](#Bias-Variance-Trade-Off)\n",
    "3. [Ridge Regression](#Ridge-Regression)\n",
    "4. [Lasso Regression](#Lasso-Regression)\n",
    "5. [Elastic Net](#Elastic-Net)\n",
    "6. [Conclusion](#Conclusion)\n",
    "\n",
    "## Introduction\n",
    "Regularization techniques are essential in regression analysis to prevent overfitting, especially when dealing with high-dimensional data. The MAP estimation provides a Bayesian framework to incorporate prior information into the model, resulting in regularization.\n",
    "\n",
    "## Bias-Variance Trade-Off\n",
    "In regression analysis, two critical characteristics of estimators are bias and variance:\n",
    "\n",
    "- **Bias**: The difference between the expected estimator and the true parameter value. It measures the systematic error introduced by the model.\n",
    "- **Variance**: The variability of the estimator due to different training data samples. It measures how much the estimator fluctuates around its expected value.\n",
    "\n",
    "The total error of a model can be decomposed into three parts:\n",
    "1. **Bias**: Error due to systematic deviations from the true parameter.\n",
    "2. **Variance**: Error due to fluctuations around the expected value of the estimator.\n",
    "3. **Irreducible Error**: Noise inherent in the data that cannot be explained by the model.\n",
    "\n",
    "Mathematically, the expected prediction error for a given data point \\( x \\) can be expressed as:\n",
    "$$\n",
    "\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{Bias}^2(\\hat{f}(x)) + \\text{Var}(\\hat{f}(x)) + \\sigma^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y$ is the true value.\n",
    "- $\\hat{f}(x)$ is the predicted value.\n",
    "- $\\sigma^2$ is the irreducible error (variance of the noise).\n",
    "\n",
    "### Bias-Variance Trade-Off in Linear Regression\n",
    "\n",
    "Consider the linear regression model:\n",
    "$$\n",
    "y = X\\mathbf{w} + \\epsilon\n",
    "$$\n",
    "where \\( \\epsilon \\sim \\mathcal{N}(0, \\sigma^2) \\).\n",
    "\n",
    "#### Bias of the OLS Estimator\n",
    "The Ordinary Least Squares (OLS) estimator is given by:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{OLS}} = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "For the OLS estimator, the bias is:\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\mathbf{w}}_{\\text{OLS}}) = \\mathbb{E}[\\hat{\\mathbf{w}}_{\\text{OLS}}] - \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Since the OLS estimator is unbiased, we have:\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\mathbf{w}}_{\\text{OLS}}) = 0\n",
    "$$\n",
    "\n",
    "#### Variance of the OLS Estimator\n",
    "The variance of the OLS estimator is given by:\n",
    "$$\n",
    "\\text{Var}(\\hat{\\mathbf{w}}_{\\text{OLS}}) = \\mathbb{E}[(\\hat{\\mathbf{w}}_{\\text{OLS}} - \\mathbb{E}[\\hat{\\mathbf{w}}_{\\text{OLS}}])(\\hat{\\mathbf{w}}_{\\text{OLS}} - \\mathbb{E}[\\hat{\\mathbf{w}}_{\\text{OLS}}])^T]\n",
    "$$\n",
    "\n",
    "Substituting the OLS estimator:\n",
    "$$\n",
    "\\text{Var}(\\hat{\\mathbf{w}}_{\\text{OLS}}) = \\sigma^2 (X^TX)^{-1}\n",
    "$$\n",
    "\n",
    "#### Total Error\n",
    "The total error in the OLS estimation can be expressed as:\n",
    "$$\n",
    "\\mathbb{E}[(\\mathbf{w} - \\hat{\\mathbf{w}}_{\\text{OLS}})^T(\\mathbf{w} - \\hat{\\mathbf{w}}_{\\text{OLS}})] = \\text{Bias}^2(\\hat{\\mathbf{w}}_{\\text{OLS}}) + \\text{Var}(\\hat{\\mathbf{w}}_{\\text{OLS}})\n",
    "$$\n",
    "\n",
    "Since the OLS estimator is unbiased, the total error is dominated by the variance:\n",
    "$$\n",
    "\\mathbb{E}[(\\mathbf{w} - \\hat{\\mathbf{w}}_{\\text{OLS}})^T(\\mathbf{w} - \\hat{\\mathbf{w}}_{\\text{OLS}})] = \\sigma^2 \\text{Tr}((X^TX)^{-1})\n",
    "$$\n",
    "\n",
    "### Effect of Regularization\n",
    "\n",
    "Regularization techniques introduce bias into the model to reduce the variance, resulting in a lower total error. Let's see how this works for Ridge Regression and Lasso Regression.\n",
    "\n",
    "#### Ridge Regression\n",
    "In Ridge Regression, we add an $\\ell_2$ penalty to the loss function:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{ridge}} = \\arg\\min_{\\mathbf{w}} \\left[ \\| y - X\\mathbf{w} \\|_2^2 + \\lambda \\| \\mathbf{w} \\|_2^2 \\right]\n",
    "$$\n",
    "\n",
    "The Ridge Regression estimator is:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{ridge}} = (X^TX + \\lambda I)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "- **Bias**: The bias increases as $\\lambda$ increases because the penalty term shrinks the coefficients towards zero.\n",
    "- **Variance**: The variance decreases as $\\lambda$ increases because the penalty term reduces the flexibility of the model.\n",
    "\n",
    "#### Lasso Regression\n",
    "In Lasso Regression, we add an $\\ell_1$ penalty to the loss function:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{lasso}} = \\arg\\min_{\\mathbf{w}} \\left[ \\| y - X\\mathbf{w} \\|_2^2 + \\lambda \\| \\mathbf{w} \\|_1 \\right]\n",
    "$$\n",
    "\n",
    "- **Bias**: The bias increases as $\\lambda$ increases because the penalty term shrinks the coefficients towards zero and can set some coefficients exactly to zero.\n",
    "- **Variance**: The variance decreases as $\\lambda$ increases because the penalty term reduces the flexibility of the model.\n",
    "\n",
    "By adjusting the regularization parameter $\\lambda$, we can find a balance between bias and variance that minimizes the total error.\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "### Traditional Ridge Regression\n",
    "In Ridge Regression, we penalize the size of parameter estimates by adding an $\\ell_2$ penalty term to the loss function.\n",
    "\n",
    "The Ridge Regression estimator is given by:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{ridge}} = (X^TX + \\lambda I)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter.\n",
    "\n",
    "### Bayesian Perspective: MAP Formulation\n",
    "From a Bayesian perspective, Ridge Regression can be seen as the MAP estimate with a Gaussian prior on the parameters.\n",
    "\n",
    "- **Prior Distribution**: \n",
    "$$\n",
    "\\mathbf{w} \\sim \\mathcal{N}(0, \\tau^2 I)\n",
    "$$\n",
    "\n",
    "- **Likelihood**: \n",
    "$$\n",
    "y | X, \\mathbf{w} \\sim \\mathcal{N}(X\\mathbf{w}, \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "Using Bayes' theorem, the posterior distribution of $\\mathbf{w}$ is:\n",
    "$$\n",
    "p(\\mathbf{w} | y, X) \\propto p(y | X, \\mathbf{w}) p(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "Combining the prior and likelihood, we get:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{MAP}} = (\\mathbf{X}^T \\mathbf{X} + \\lambda I)^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "where $\\lambda = \\frac{\\sigma^2}{\\tau^2}$.\n",
    "\n",
    "## Lasso Regression\n",
    "\n",
    "### Traditional Lasso Regression\n",
    "In Lasso Regression, we penalize the size of parameter estimates by adding an $\\ell_1$ penalty term to the loss function.\n",
    "\n",
    "The Lasso Regression estimator is given by:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{lasso}} = \\arg\\min_{\\mathbf{w}} \\left[ \\| y - X\\mathbf{w} \\|_2^2 + \\lambda \\| \\mathbf{w} \\|_1 \\right]\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter.\n",
    "\n",
    "### Bayesian Perspective: MAP Formulation\n",
    "From a Bayesian perspective, Lasso Regression can be seen as the MAP estimate with a Laplace prior on the parameters.\n",
    "\n",
    "- **Prior Distribution**: \n",
    "$$\n",
    "\\mathbf{w} \\sim \\text{Laplace}(0, b)\n",
    "$$\n",
    "\n",
    "- **Likelihood**: \n",
    "$$\n",
    "y | X, \\mathbf{w} \\sim \\mathcal{N}(X\\mathbf{w}, \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "Using Bayes' theorem, the posterior distribution of $\\mathbf{w}$ is:\n",
    "$$\n",
    "p(\\mathbf{w} | y, X) \\propto p(y | X, \\mathbf{w}) p(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "Combining the prior and likelihood, we get:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{w}} \\left[ \\frac{1}{2\\sigma^2} \\| y - X\\mathbf{w} \\|_2^2 + \\frac{1}{b} \\| \\mathbf{w} \\|_1 \\right]\n",
    "$$\n",
    "\n",
    "where $\\lambda = \\frac{\\sigma^2}{b}$.\n",
    "\n",
    "## Elastic Net\n",
    "\n",
    "### Traditional Elastic Net\n",
    "Elastic Net combines the penalties of Ridge and Lasso Regression. The loss function is a mixture of $\\ell_1$ and $\\ell_2$ penalties.\n",
    "\n",
    "The Elastic Net estimator is given by:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{elastic}} = \\arg\\min_{\\mathbf{w}} \\left[ \\| y - X\\mathbf{w} \\|_2^2 + \\lambda_1 \\| \\mathbf{w} \\|_1 + \\lambda_2 \\| \\mathbf{w} \\|_2^2 \\right]\n",
    "$$\n",
    "\n",
    "where $\\lambda_1$ and $\\lambda_2$ are regularization parameters.\n",
    "\n",
    "### Bayesian Perspective: MAP Formulation\n",
    "From a Bayesian perspective, Elastic Net can be seen as the MAP estimate with a combined Gaussian and Laplace prior on the parameters.\n",
    "\n",
    "- **Prior Distribution**: \n",
    "$$\n",
    "\\mathbf{w} \\sim \\text{Gaussian-Laplace}(0, \\tau^2, b)\n",
    "$$\n",
    "\n",
    "- **Likelihood**: \n",
    "$$\n",
    "y | X, \\mathbf{w} \\sim \\mathcal{N}(X\\mathbf{w}, \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "Using Bayes' theorem, the posterior distribution of $\\mathbf{w}$ is:\n",
    "$$\n",
    "p(\\mathbf{w} | y, X) \\propto p(y | X, \\mathbf{w}) p(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "Combining the prior and likelihood, we get:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{MAP}} = \\arg\\min_{\\mathbf{w}} \\left[ \\frac{1}{2\\sigma^2} \\| y - X\\mathbf{w} \\|_2^2 + \\frac{1}{b} \\| \\mathbf{w} \\|_1 + \\frac{1}{2\\tau^2} \\| \\mathbf{w} \\|_2^2 \\right]\n",
    "$$\n",
    "\n",
    "where $\\lambda_1 = \\frac{\\sigma^2}{b}$ and $\\lambda_2 = \\frac{\\sigma^2}{\\tau^2}$.\n",
    "\n",
    "## Conclusion\n",
    "In this notebook, we explored Ridge, Lasso, and Elastic Net regressions from both traditional and Bayesian perspectives using the MAP formulation. We derived the MAP estimates for each regularization technique and provided practical examples using Python's `scikit-learn` library.\n",
    "\n",
    "Regularization is a powerful tool to prevent overfitting and improve the predictive performance of regression models. By incorporating prior information into our models, we can achieve better estimates and more reliable predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
