{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Boosting\n",
    "\n",
    "Boosting is a powerful ensemble technique that aims to improve the accuracy of weak learners by combining them into a strong learner. Unlike bagging, where models are trained independently, boosting trains models sequentially. Each new model focuses on correcting the errors made by the previous models.\n",
    "\n",
    "### Key Concepts of Boosting\n",
    "\n",
    "- **Weak Learner:** A model that performs slightly better than random guessing.\n",
    "- **Sequential Training:** Each subsequent model is trained to correct the errors of the previous models.\n",
    "- **Weighted Voting:** The final prediction is a weighted combination of the predictions from all models.\n",
    "\n",
    "Boosting techniques can be applied to both classification and regression tasks. The most popular boosting algorithms are **AdaBoost** and **Gradient Boosting**.\n",
    "\n",
    "## AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost is one of the earliest and most popular boosting algorithms. It combines multiple weak classifiers to form a strong classifier by adjusting the weights of the training examples based on the performance of the classifiers.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given a dataset $D = \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^N$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, +1\\}$, the AdaBoost algorithm works as follows:\n",
    "\n",
    "1. **Initialize Weights:** Initialize the weight of each training example to be equal: $w_i^{(1)} = \\frac{1}{N}$.\n",
    "   \n",
    "2. **Train Weak Classifiers:** For each round $t = 1, 2, \\dots, T$:\n",
    "   - Train a weak classifier $h_t(\\mathbf{x})$ using the weighted training data.\n",
    "   - Calculate the error rate $\\epsilon_t$ of the classifier $h_t(\\mathbf{x})$:\n",
    "   $$\n",
    "   \\epsilon_t = \\sum_{i=1}^N w_i^{(t)} I(h_t(\\mathbf{x}_i) \\neq y_i)\n",
    "   $$\n",
    "   - Compute the classifier's weight $\\alpha_t$:\n",
    "   $$\n",
    "   \\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right)\n",
    "   $$\n",
    "   - Update the weights of the training examples:\n",
    "   $$\n",
    "   w_i^{(t+1)} = w_i^{(t)} \\exp(-\\alpha_t y_i h_t(\\mathbf{x}_i))\n",
    "   $$\n",
    "   Normalize the weights so that they sum to 1.\n",
    "\n",
    "3. **Final Classifier:** The final strong classifier $H(\\mathbf{x})$ is a weighted combination of the weak classifiers:\n",
    "   $$\n",
    "   H(\\mathbf{x}) = \\text{sign} \\left( \\sum_{t=1}^T \\alpha_t h_t(\\mathbf{x}) \\right)\n",
    "   $$\n",
    "\n",
    "#### Example: Implementing AdaBoost\n",
    "\n",
    "Let's implement AdaBoost using decision stumps (one-level decision trees) as weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regressor Mean Squared Error: 5853.62\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_regression(n_samples=500, n_features=20, n_informative=15, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gboost_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gboost = gboost_model.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse_gboost = mean_squared_error(y_test, y_pred_gboost)\n",
    "print(f'Gradient Boosting Regressor Mean Squared Error: {mse_gboost:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants of Boosting\n",
    "\n",
    "Boosting has many variants that build upon the basic concepts of AdaBoost and Gradient Boosting. Some notable variants include:\n",
    "\n",
    "### 1. **LogitBoost**\n",
    "\n",
    "LogitBoost is an extension of AdaBoost designed to minimize the logistic loss function. It is particularly effective in binary classification problems.\n",
    "\n",
    "### 2. **XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "XGBoost is an advanced implementation of gradient boosting that is highly optimized for speed and performance. It includes several regularization techniques to prevent overfitting and is widely used in machine learning competitions.\n",
    "\n",
    "### 3. **CatBoost**\n",
    "\n",
    "CatBoost is a gradient boosting algorithm specifically designed to handle categorical features efficiently. It uses ordered boosting to reduce prediction shift and provides state-of-the-art performance on many tasks.\n",
    "\n",
    "### 4. **LightGBM**\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient, offering faster training speed and lower memory usage.\n",
    "\n",
    "These variants offer additional features and optimizations, making boosting a versatile and powerful technique in the machine learning toolbox.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this section, we've explored the fundamentals of boosting techniques, focusing on AdaBoost and Gradient Boosting. These methods have revolutionized machine learning by allowing weak learners to be combined into strong learners, improving both accuracy and robustness.\n",
    "\n",
    "### Summary:\n",
    "- **AdaBoost**: Sequentially combines weak classifiers by focusing on hard-to-classify instances.\n",
    "- **Gradient Boosting**: Generalizes boosting to optimize any differentiable loss function, making it suitable for a wide range of tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "1. Implement LogitBoost using decision stumps as weak learners and compare its performance with AdaBoost.\n",
    "2. Explore the effect of different learning rates in Gradient Boosting on a regression task. What trade-offs do you observe?\n",
    "3. Experiment with XGBoost on a real-world dataset. Compare its performance and training time with traditional Gradient Boosting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
