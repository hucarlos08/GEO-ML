{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec18c511",
   "metadata": {},
   "source": [
    "# Introduction to Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c24e16",
   "metadata": {},
   "source": [
    "\n",
    "## What is Ensemble Learning?\n",
    "\n",
    "Ensemble learning is a powerful machine learning paradigm where multiple models, often referred to as \"learners\" or \"weak learners,\" are combined to solve a particular problem. The idea behind ensemble methods is that a group of weak learners can come together to form a strong learner, achieving better performance than any single model could on its own.\n",
    "\n",
    "Ensemble methods are particularly effective in improving the accuracy and robustness of predictions. They are widely used in classification, regression, and other predictive tasks.\n",
    "\n",
    "### Why Use Ensemble Learning?\n",
    "\n",
    "- **Reduction of Overfitting:** By averaging the predictions of multiple models, ensemble methods can reduce the variance and prevent overfitting, leading to better generalization on unseen data.\n",
    "- **Improved Accuracy:** Combining multiple models often results in improved accuracy compared to a single model, as errors from individual models may cancel each other out.\n",
    "- **Robustness:** Ensemble methods are generally more robust to noisy data and model mis-specifications.\n",
    "\n",
    "In this section, we will explore two of the most popular ensemble techniques: **Bagging** and **Boosting**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc33c65",
   "metadata": {},
   "source": [
    "\n",
    "## Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Bagging is an ensemble technique designed to improve the stability and accuracy of machine learning algorithms. It does this by training multiple instances of a model on different subsets of the training data and then averaging the predictions.\n",
    "\n",
    "### How Bagging Works\n",
    "\n",
    "1. **Bootstrap Sampling:** From the original dataset, multiple subsets are created using bootstrap sampling. This means each subset is generated by randomly selecting samples from the original dataset with replacement.\n",
    "2. **Model Training:** A separate model is trained on each bootstrap sample. These models can be of the same type or different types.\n",
    "3. **Aggregation:** For classification tasks, the predictions of the models are combined using a majority vote. For regression tasks, the predictions are averaged.\n",
    "\n",
    "Mathematically, for a new input \\( \\mathbf{x} \\), the bagging prediction is given by:\n",
    "\\[\n",
    "\\hat{y} = \f\n",
    "rac{1}{M} \\sum_{m=1}^{M} h_m(\\mathbf{x})\n",
    "\\]\n",
    "where \\( h_m(\\mathbf{x}) \\) is the prediction of the \\( m \\)-th model, and \\( M \\) is the total number of models.\n",
    "\n",
    "### Example: Bagging with Decision Trees\n",
    "\n",
    "Let's implement a simple bagging ensemble using decision trees as the base learners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Bagging classifier with decision trees as base learners\n",
    "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Bagging Classifier Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea34bbe",
   "metadata": {},
   "source": [
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting is another powerful ensemble technique. Unlike bagging, where models are trained independently, boosting models are trained sequentially. Each new model focuses on correcting the errors made by the previous models.\n",
    "\n",
    "### How Boosting Works\n",
    "\n",
    "1. **Initialize Weights:** All training examples are assigned equal weights.\n",
    "2. **Train Models Sequentially:** Each model is trained to correct the mistakes of the previous one by focusing more on the incorrectly classified instances.\n",
    "3. **Combine Models:** The final prediction is a weighted sum of the predictions from all models.\n",
    "\n",
    "A common boosting algorithm is **AdaBoost**. In AdaBoost, the weight of each incorrectly classified instance is increased so that the subsequent model pays more attention to those instances.\n",
    "\n",
    "Mathematically, the prediction for a new input $ \\mathbf{x} $ in AdaBoost is given by:\n",
    "$$\n",
    "\\hat{y} = \\text{sign} \\left( \\sum_{m=1}^{M} \\alpha_m h_m(\\mathbf{x}) \\right)\n",
    "$$\n",
    "where $\\alpha_m$ is the weight assigned to the $ m $-th model's prediction, and $ h_m(\\mathbf{x}) $ is the prediction of the $ m $-th model.\n",
    "\n",
    "### Example: Boosting with AdaBoost\n",
    "\n",
    "Let's implement a simple boosting ensemble using AdaBoost with decision trees as the base learners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create an AdaBoost classifier with decision trees as base learners\n",
    "boosting_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "boosting_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_boost = boosting_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_boost = accuracy_score(y_test, y_pred_boost)\n",
    "print(f'AdaBoost Classifier Accuracy: {accuracy_boost:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dacdd4",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this section, we've explored the fundamental concepts of ensemble learning, focusing on bagging and boosting techniques. Both methods have their own strengths and are widely used in various machine learning tasks. \n",
    "\n",
    "### Summary:\n",
    "- **Bagging** is effective at reducing variance and overfitting by averaging predictions from multiple models.\n",
    "- **Boosting** focuses on reducing bias by giving more attention to hard-to-classify instances.\n",
    "\n",
    "These techniques lay the foundation for more advanced ensemble methods and are an essential part of any machine learning practitioner's toolkit.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
